{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. to handle the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import csv\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# to visualize the data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# To preprocess the data\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder,OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "# import iterative imputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "#for classification tasks\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_absolute_error,mean_squared_error,r2_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# ignore warnings   \n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)  # None means unlimited rows\n",
    "pd.set_option('display.max_columns', None)  # None means unlimited columns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "godavari_df = pd.read_csv('INDUS_GODAVARI/samples.csv', delimiter=';')\n",
    "indus_df = pd.read_csv('INDUS_GODAVARI/samples_indus.csv', delimiter=';')\n",
    "\n",
    "\n",
    "# Concatenate the rows\n",
    "merged_df = pd.concat([godavari_df, indus_df], ignore_index=True)\n",
    "\n",
    "# Write the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('INDUS_GODAVARI/merged_samples.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(\"INDUS_GODAVARI/samples.csv\", delimiter=\";\")\n",
    "# Drop the 6th, 7th, and 9th columns\n",
    "df = df.drop(df.columns[[5, 6, 8]], axis=1)\n",
    "\n",
    "\n",
    "# Extract unique parameter names from the 5th column\n",
    "unique_parameters = df.iloc[:, 4].unique()\n",
    "\n",
    "# Create a dictionary to hold data for each parameter\n",
    "param_data = {}\n",
    "for param in unique_parameters:\n",
    "    param_data[param] = df[df.iloc[:, 4] == param].iloc[:, 5]\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "param_df = pd.DataFrame(param_data)\n",
    "\n",
    "# Merge the original DataFrame with the parameter DataFrame\n",
    "merged_df = pd.concat([df.iloc[:, :-2], param_df, df.iloc[:, -1]], axis=1)\n",
    "merged_df = merged_df.drop(merged_df.columns[4], axis=1)\n",
    "merged_df.columns = ['Station', 'Date', 'Time', 'Index', 'Alk-Tot', 'TP', 'O2-Dis', 'BOD', 'TOTCOLI', 'TDS', 'pH', 'B-Dis', 'TEMP', 'H-T', 'Na-Dis', 'TURB', 'Ca-Dis', 'COD', 'SO4-Dis', 'Cl-Dis', 'Mg-Dis', 'NOxN', 'TKN', 'FDS', 'Q-Inst', 'FECALCOLI', 'NO3N', 'NH4N', 'TSS', 'NH3N', 'NO2N', 'F-Dis', 'SAR', 'K-Tot', 'DIELDRIN', 'ENDOSULFANII', '24DDT', 'Cd-Tot', 'As-Tot', 'Ni-Tot', 'Hg-Tot', 'Cu-Tot', '24D', 'ENDOSULFANI', 'Pb-Tot', 'Fe-Tot', 'Zn-Tot', 'Cr-Tot', 'DDT', 'ALDRIN', '44DDT', 'BHC-gamma', 'FECALSTREP', 'Pb-Dis', 'TS', 'Fe-Dis', 'Cd-Dis', 'As-Dis', 'Hg-Dis', 'Mn-Dis', 'DRP', 'K-Dis', 'HCO3', 'CO3', 'H-Ca', 'SiO2-Dis', 'Alk-Phen', 'O2-Dis-Sat', 'TRANS', 'Cr-Dis', 'Cu-Dis', 'Ni-Dis', 'Zn-Dis', 'Al-Dis', 'TOC', 'Quality']\n",
    "# Print the transformed DataFrame\n",
    "print(merged_df)\n",
    "\n",
    "\n",
    "merged_df.to_csv('INDUS_GODAVARI/merged_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('INDUS_GODAVARI/merged_file.csv')\n",
    "\n",
    "# Remove rows with 'Unknown' values\n",
    "data_known = data[data['Quality'] != 'Unknown']\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "# Pie plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "data_known['Quality'].value_counts().plot.pie(autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of Quality')\n",
    "plt.ylabel('')\n",
    "plt.show()\n",
    "\n",
    "# Preprocessing\n",
    "# Encode the 'Quality' column into numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "data_known['Quality'] = label_encoder.fit_transform(data_known['Quality'])\n",
    "\n",
    "# Split the known data into features and target variable\n",
    "X_known = data_known.drop(columns=['Quality'])\n",
    "y_known = data_known['Quality']\n",
    "\n",
    "# Define categorical and numeric features\n",
    "categorical_features = X_known.select_dtypes(include=['object']).columns.tolist()\n",
    "numeric_features = X_known.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Create pipelines for preprocessing\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numeric', numeric_pipeline, numeric_features),\n",
    "    ('categorical', categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# Combine preprocessing with SMOTE\n",
    "pipeline = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "])\n",
    "\n",
    "# Apply preprocessing and SMOTE\n",
    "X_resampled, y_resampled = pipeline.fit_resample(X_known, y_known)\n",
    "\n",
    "# Split resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the multinomial logistic regression model\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "# with open('output.txt', 'w') as file:\n",
    "#     # Write the desired content to the file\n",
    "#     file.write(y_test.to_string(index=False))\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_known = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on known data:\", accuracy_known)\n",
    "print(\"Classification Report on known data:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "unknown_data = data[data['Quality'] == 'Unknown']\n",
    "X_unknown = unknown_data.drop(columns=['Quality'])\n",
    "\n",
    "# Apply preprocessing on unknown data\n",
    "X_unknown_processed = pipeline[:-1].transform(X_unknown)\n",
    "\n",
    "# Predict unknown values\n",
    "unknown_predictions = model.predict(X_unknown_processed)\n",
    "unknown_data['Predicted_Quality'] = label_encoder.inverse_transform(unknown_predictions)\n",
    "\n",
    "# Save X_unknown and their predicted values into a new CSV file\n",
    "unknown_data.to_csv('INDUS_GODAVARI/predicted_unknown.csv', index=False)\n",
    "print(\"Predicted Quality for Unknown values saved in predicted_unknown.csv file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data \n",
    "df = pd.read_csv(\"combinedData_Kaggle/water_potability.csv\")\n",
    "# print first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "plt.figure(figsize=(6,6))\n",
    "# Pie plot\n",
    "df['Potability'].value_counts().plot.pie(explode=[0.1,0.1],\n",
    "                    autopct='%1.1f%%', shadow=True,\n",
    "                    textprops={'fontsize':16}).set_title(\"Target distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "variables = ['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity',\n",
    "             'Organic_carbon', 'Trihalomethanes', 'Turbidity']\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))\n",
    "\n",
    "# Flatten the axes for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through each variable and create histograms\n",
    "for i, var in enumerate(variables):\n",
    "    ax = axes[i]\n",
    "    sns.histplot(df[var], kde=True, ax=ax)  \n",
    "    ax.axvline(df[var].mean(), color='red', linestyle='--', label='Mean')\n",
    "    ax.axvline(df[var].median(), color='blue', linestyle='--', label='Median')\n",
    "    \n",
    "    # Annotate plot with mean and median\n",
    "    ax.annotate(f'Mean: {df[var].mean():.2f}\\nMedian: {df[var].median():.2f}',\n",
    "                xy=(0.05, 0.95), xycoords='axes fraction', ha='left', va='top')\n",
    "    \n",
    "    ax.set_title(f'Histogram with KDE for {var}')\n",
    "    ax.set_xlabel(var)\n",
    "    ax.legend()\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check the correlation of all columns\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df.corr(),annot=True,cmap='coolwarm',linewidths=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dew_point(temperature, relative_humidity):\n",
    "    T = temperature\n",
    "    RH = relative_humidity / 100.0\n",
    "    \n",
    "    # Magnus formula constants\n",
    "    a = 17.27\n",
    "    b = 237.7\n",
    "    \n",
    "    # Calculate saturation vapor pressure\n",
    "    alpha = ((a * T) / (b + T)) + np.log(RH)\n",
    "    saturation_vapor_pressure = b * (np.exp(alpha) / (a - np.exp(alpha)))\n",
    "    \n",
    "    # Calculate dew point temperature\n",
    "    dew_point_temperature = (b * alpha) / (a - alpha)\n",
    "    \n",
    "    return dew_point_temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming merged_df is your DataFrame containing the features\n",
    "merged_df = pd.read_csv('INDUS_GODAVARI/merged_file.csv')  # Load your dataset here\n",
    "\n",
    "# Drop non-feature columns\n",
    "features = merged_df.drop(columns=['Station', 'Date', 'Time', 'Quality'])\n",
    "\n",
    "# Impute missing values using mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "features_imputed = imputer.fit_transform(features)\n",
    "\n",
    "# Standardize the features\n",
    "features_standardized = (features_imputed - features_imputed.mean()) / features_imputed.std()\n",
    "\n",
    "# Perform PCA to reduce dimensionality\n",
    "pca = PCA(n_components=2)  # You can adjust the number of components\n",
    "pca_result = pca.fit_transform(features_standardized)\n",
    "\n",
    "# Perform clustering\n",
    "kmeans = KMeans(n_clusters=3)  # You can adjust the number of clusters\n",
    "clusters = kmeans.fit_predict(pca_result)\n",
    "\n",
    "# # Add cluster labels to the DataFrame\n",
    "# merged_df['Cluster'] = clusters\n",
    "\n",
    "# Group by 'Station' and compute the mean of 'TEMP' and 'H_T', ignoring NaN values\n",
    "station_means = merged_df.groupby('Station')[['TEMP', 'H-T']].mean().dropna()\n",
    "\n",
    "# Convert the result to a dictionary with station as key and (temperature, H_T) tuple as value\n",
    "station_means_dict = station_means.apply(tuple, axis=1).to_dict()\n",
    "\n",
    "# Compute dew point temperature for each sample using PCA components\n",
    "dew_point_temperatures = []\n",
    "for i in range(len(merged_df)):\n",
    "    station=merged_df.at[i,'Station']\n",
    "    if station in station_means_dict:\n",
    "        temperature = station_means_dict[station][0]  # Assuming 'TEMP' is temperature column\n",
    "        relative_humidity = station_means_dict[station][1] # Assuming 'H-T' is relative humidity column\n",
    "        dew_point_temp = compute_dew_point(temperature, relative_humidity)\n",
    "        dew_point_temperatures.append(dew_point_temp)\n",
    "    else:   \n",
    "        dew_point_temperatures.append(np.nan)\n",
    "        \n",
    "# Assuming dew_point_temperatures is your list of values\n",
    "output_file_path = \"INDUS_GODAVARI/dew_point_temperatures.txt\"\n",
    "\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for temperature in dew_point_temperatures:\n",
    "        f.write(str(temperature) + '\\n')\n",
    "\n",
    "\n",
    "# Add predicted dew point temperatures to the DataFrame\n",
    "merged_df['Predicted Dew Point'] = dew_point_temperatures\n",
    "merged_df.to_csv('INDUS_GODAVARI/Predicted_dew.csv',index=False)\n",
    "\n",
    "# Visualize explained variance ratio to decide on the number of components\n",
    "plt.bar(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.show()\n",
    "\n",
    "# Visualize clusters in PCA space\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], c=clusters, cmap='viridis')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA with Cluster Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv('Canada/Canada.csv', encoding='latin1')\n",
    "\n",
    "# Pivot the data to have each unique VARIABLE as a separate column\n",
    "pivoted_data = data.pivot_table(index=['PROV_TERR','SITE_NO','SITE_NAME_NOM', 'DATE'],\n",
    "                                columns='VARIABLE',\n",
    "                                values='VALUE_VALEUR').reset_index()\n",
    "\n",
    "# Select only the desired columns\n",
    "selected_columns = ['PROV_TERR','SITE_NO','SITE_NAME_NOM','DATE', 'TEMPERATURE (WATER)', 'PH', 'TURBIDITY']\n",
    "\n",
    "# Filter the pivoted data directly\n",
    "filtered_data = pivoted_data[selected_columns]\n",
    "\n",
    "columns_to_fill = ['TEMPERATURE (WATER)', 'PH', 'TURBIDITY']\n",
    "for column in columns_to_fill:\n",
    "    filtered_data[column].fillna(filtered_data[column].mean(), inplace=True)\n",
    "    \n",
    "filtered_data['DATE'] = pd.to_datetime(filtered_data['DATE'], dayfirst=True)\n",
    "filtered_data = filtered_data.sort_values(by='DATE')\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "filtered_data.to_csv('Canada/filtered_canada_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv('Canada/filtered_canada_data.csv')\n",
    "data=data.drop(columns=['DATE','PROV_TERR','SITE_NO'])\n",
    "\n",
    "grouped_data = data.groupby('SITE_NAME_NOM').mean().reset_index()\n",
    "\n",
    "# Draw box plot for Turbidity\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(grouped_data['TURBIDITY'])\n",
    "plt.title('Box Plot of Turbidity')\n",
    "plt.ylabel('Turbidity')\n",
    "plt.show()\n",
    "\n",
    "# Draw box plot for WaterTemperature\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(grouped_data['TEMPERATURE (WATER)'])\n",
    "plt.title('Box Plot of Water Temperature')\n",
    "plt.ylabel('Water Temperature')\n",
    "plt.show()\n",
    "\n",
    "# Draw box plot for Rainfall\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(grouped_data['PH'])\n",
    "plt.title('Box Plot of pH')\n",
    "plt.ylabel('pH')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('Canada/filtered_canada_data.csv')\n",
    "\n",
    "# Drop 'SITE_NO' and 'PROV_TERR'\n",
    "df.drop(columns=['SITE_NO', 'PROV_TERR'], inplace=True)\n",
    "\n",
    "# Convert 'DATE' column to datetime\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "\n",
    "# Find the most frequent SITE_NAME_NOM\n",
    "most_frequent_site = 'SALMON RIVER AT HYDER, ALASKA'\n",
    "\n",
    "# Filter the DataFrame for the most frequent site\n",
    "most_frequent_df = df[df['SITE_NAME_NOM'] == most_frequent_site]\n",
    "\n",
    "# Plot TEMPERATURE (WATER), PH, and TURBIDITY individually as a function of time\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.lineplot(x='DATE', y='TEMPERATURE (WATER)', data=most_frequent_df)\n",
    "plt.title('Temperature (Water)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.lineplot(x='DATE', y='PH', data=most_frequent_df)\n",
    "plt.title('pH')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('pH')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.lineplot(x='DATE', y='TURBIDITY', data=most_frequent_df)\n",
    "plt.title('Turbidity')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Turbidity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Canada/filtered_canada_data.csv')\n",
    "unique_values = df['SITE_NAME_NOM'].unique()\n",
    "# print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_coordinates={\n",
    "    'MERSEY R. BELOW MILL FALLS SW OF MAITLAND BRIDGE':[44.408160,-65.233150],\n",
    "    'ASSINIBOINE RIVER AT HWY 8 BRIDGE':[46.289870,-66.592980],\n",
    "    'BOW RIVER AT HIGHWAY 1 ABOVE LAKE LOUISE':[51.334970,-116.049740],\n",
    "    'BOW RIVER ABOUT 4.5 KM ABOVE CANMORE':[51.088425,-115.350551],\n",
    "    'CARROT RIVER NEAR TURNBERRY,':[53.279560,-103.587550],\n",
    "    'ATHABASCA RIVER AT HIGHWAY #16 BELOW SNARING RIVER':[54.729170,-113.279200],\n",
    "    'SALMON RIVER AT HYDER, ALASKA':[45.367690,-63.230620],\n",
    "    'KOOTENAY RIVER AT KOOTENAY CROSSING':[49.102690,-116.554240],\n",
    "    \"BEAVER RIVER AT BEAVER CROSSING,\":[44.014960,-66.149250],\n",
    "    'WINNIPEG RIVER AT POINTE DU BOIS,':[50.294330,-95.544890],\n",
    "    'PEMBINA RIVER AT WINDYGATES, MANITOBA':[52.105820,-101.266100],\n",
    "    'RED RIVER AT EMERSON, MANITOBA':[49.717560,-97.126360],\n",
    "    'LIARD RIVER AT UPPER CROSSING':[60.051760,-128.908480],\n",
    "    'SOURIS RIVER NEAR WESTHOPE':[46.373180,-62.278660],\n",
    "    'ILLECILLEWAET RIVER AT GLACIER NATIONAL PARK ENTRANCE':[-43.391920,170.181770],\n",
    "    'LIARD RIVER AT FORT LIARD':[61.742670,-121.219670],\n",
    "    'MACKENZIE RIVER ABOVE ARCTIC RED RIVER':[47.801200,-103.213180],\n",
    "    'HAY RIVER NEAR ALBERTA/NWT BORDER':[60.810460,-115.788150],\n",
    "    'LIARD RIVER NEAR THE MOUTH':[59.514120,-126.364610],\n",
    "    'MACKENZIE RIVER AT STRONG POINT':[44.574490,-64.160770],\n",
    "    'GREAT BEAR RIVER AT OUTLET OF GREAT BEAR LAKE':[27.889740,-80.511350],\n",
    "    'MACKENZIE RIVER AT NORMAN WELLS':[65.270710,-126.765710],\n",
    "    'SASKATCHEWAN RIVER ABOVE CARROT RIVER':[53.279560,-103.587550],\n",
    "    'RED DEER RIVER NEAR BINDLOSS, ALBERTA':[46.140170,-77.553650],\n",
    "    'SOUTH SASKATCHEWAN RIVER AT HWY 41,':[49.768070,-94.392340],\n",
    "    'NORTH SASKATCHEWAN RIVER AT HIGHWAY #17 BRIDGE':[46.793590,-84.391110],\n",
    "    'COLD RIVER AT OUTLET OF COLD LAKE':[44.622340,-84.923290],\n",
    "    'SAINT-LAURENT À LÉVIS':[46.789440,-71.194760],\n",
    "    'DES OUTAOUAIS À CARILLON':[45.682260,-76.622480],\n",
    "    \"QU'APPELLE RIVER APPROX. 3.2KM. SOUTH\":[50.769610,-103.790310],\n",
    "    'RED DEER RIVER AT ERWOOD,':[46.140170,-77.553650],\n",
    "    'NORTH SASKATCHEWAN RIVER AT WHIRLPOOL POINT':[53.834740,-107.032540],\n",
    "    'ATHABASCA RIVER ABOVE ATHABSCA FALLS':[57.182310,-111.633200],\n",
    "    'CHURCHILL RIVER BELOW WASAWAKASIK':[55.918350,-107.719200],\n",
    "    'KLONDIKE RIVER UPSTREAM OF BONANZA CREEK':[36.064950,-115.305440],\n",
    "    'SOUTH MCQUESTEN RIVER DOWNSTREAM OF FLAT CREEK':[34.681080,-80.638930],\n",
    "    'YUKON RIVER UPSTREAM OF TAKHINI RIVER':[60.876000,-135.435450],\n",
    "    'SAINT-FRANÇOIS RIVER AT PIERREVILLE':[49.994050,-97.780910],\n",
    "    'NICOLET RIVER AT NICOLET':[45.986650,-88.652580],\n",
    "    'YAMASKA RIVER, HIGHWAY 132 BRIDGE':[45.258300,-66.089250],\n",
    "    'ANNAPOLIS R. @ BRIDGE 650M SOUTH HWY 1 (WSC GAUGE)':[45.568870,-65.753310],\n",
    "    'CORNWALLIS RIVER AT BRIDGE 850 M NORTH OF HWY 1':[49.185620,-122.750140],\n",
    "    \"ST. MARY'S RIVER AT HWY 7 BRIDGE,STILLWATER\":[44.639920,-73.113450],\n",
    "    'SOUTH RIVER AT ST. ANDREWS':[45.470610,-61.942790],\n",
    "    'CHETICAMP RIVER ABOVE ROBERT BROOK (WSC GAUGE)':[46.644510,-60.947590],\n",
    "    'KELLEY RIVER AT NINE MILE FORD, GAME SANCTUARY':[44.320680,-79.887020],\n",
    "    'SHELBURNE RIVER':[42.627610,-72.736790],\n",
    "    'TUSKET RIVER AT WILSONS BRIDGE':[51.940540,-114.639360],\n",
    "    'ROSEWAY RIVER (WSC GAUGE), LOWER OHIO':[44.169675,-65.396104],\n",
    "    'LAHAVE RIVER @ WEST NORTHFIELD BRIDGE (WSC GAUGE)':[51.172908,-115.568212],\n",
    "    \"ST. LAWRENCE RIVER, WATER INTAKE OF BÉCANCOUR'S FILTRATION PLANT\":[44.617200,-75.405900],\n",
    "    'LITTLE SACKVILLE RIVER':[45.892690,-64.975100],\n",
    "    'NORTHEAST MARGAREE RIVER AT BRIDGE (WSC GAUGE)':[51.940540,-114.639360],\n",
    "    'SACKVILLE RIVER AT HWY 1 BRIDGE, BEDFORD':[44.843470,-63.799180],\n",
    "    \"ST. LAWRENCE RIVER, WATER INTAKE OF BÉCANCOUR'S FILTRATION PLANT\":[46.311578,-72.546012],\n",
    "    \"RICHELIEU RIVER, WATER INTAKE OF SOREL'S FILTRATION PLANT\":[46.039305,-73.114843],\n",
    "    \"Fleuve Saint-Laurent, Prise d'eau de l'usine de filtration de Lavaltrie\":[45.874908,-73.281964],\n",
    "    \"Rivière Saint-Maurice, Prise d'eau de l'usine de filtration de Trois-Rivières\":[46.378640,-72.614220],\n",
    "    'Rivière à la Pêche, station CABIN RPA001 (Parc national de la Mauricie)':[46.650760,-72.712080]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the original CSV file\n",
    "with open('Canada/filtered_canada_data.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    rows = list(csv_reader)\n",
    "\n",
    "# Modify the rows to include latitude and longitude\n",
    "for row in rows:\n",
    "    site_name = row['SITE_NAME_NOM']\n",
    "    if site_name in site_coordinates:\n",
    "        latitude, longitude = site_coordinates[site_name][0],site_coordinates[site_name][1]\n",
    "        row['LATITUDE'] = latitude\n",
    "        row['LONGITUDE'] = longitude\n",
    "    else:\n",
    "        row['LATITUDE'] = None\n",
    "        row['LONGITUDE'] = None\n",
    "\n",
    "# Write the modified data to a new CSV file\n",
    "fieldnames = csv_reader.fieldnames + ['LATITUDE', 'LONGITUDE']\n",
    "with open('Canada/modified_filtered_canada_data.csv', 'w', newline='') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv('Canada/modified_filtered_canada_data.csv')\n",
    "data=data.drop(columns=['PROV_TERR','SITE_NO','DATE'])\n",
    "\n",
    "# Group by 'SITE_NO' and calculate the mean for each parameter\n",
    "grouped_data = data.groupby('SITE_NAME_NOM').mean().reset_index()\n",
    "\n",
    "latitude_column = 'LATITUDE'  # Replace with your latitude column name\n",
    "longitude_column = 'LONGITUDE'  # Replace with your longitude column name\n",
    "\n",
    "# Create separate heatmaps for each parameter\n",
    "parameters = ['TURBIDITY','PH','TEMPERATURE (WATER)']\n",
    "for param in parameters:\n",
    "    # Create a map centered at the mean latitude and longitude of all sites\n",
    "    canada_map = folium.Map(location=[grouped_data[latitude_column].mean(), grouped_data[longitude_column].mean()], zoom_start=4)\n",
    "    \n",
    "    # Create a HeatMap layer for the parameter\n",
    "    heat_data = [[row[latitude_column], row[longitude_column], row[param]] for index, row in grouped_data.iterrows()]\n",
    "    HeatMap(heat_data, radius=10).add_to(canada_map)\n",
    "\n",
    "    # Save the map as an HTML file\n",
    "    canada_map.save(f'Canada/canada_heatmap_{param}.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_station_info(file_path):\n",
    "    \"\"\"\n",
    "    Extracts station information from the initial lines of the file.\n",
    "    \"\"\"\n",
    "    station_info = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#Station'):\n",
    "                key, value = line.strip().split(',', 1)\n",
    "                station_info[key] = value\n",
    "    return station_info\n",
    "\n",
    "def filter_and_combine(file_paths):\n",
    "    \"\"\"\n",
    "    Filters the datasets and combines them based on Timestamp.\n",
    "    \"\"\"\n",
    "    # Extract station information for each dataset\n",
    "    station_info = [extract_station_info(file_path) for file_path in file_paths]\n",
    "\n",
    "    # Initialize an empty DataFrame to store combined data\n",
    "    combined_data = pd.DataFrame(columns=['Station Number', 'Station Location Latitude', 'Station Location Longitude', 'Timestamp'])\n",
    "\n",
    "    # Iterate over each dataset\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        # Load the dataset skipping initial lines\n",
    "        data = pd.read_csv(file_path, skiprows=9)  # Skip 9 lines to reach data\n",
    "        \n",
    "        # Extract necessary columns\n",
    "        filtered_data = data[['#Timestamp', 'Value']]\n",
    "        \n",
    "        # Add station information\n",
    "        filtered_data['Station Number'] = station_info[i]['#Station Number']\n",
    "        filtered_data['Station Location Latitude'] = station_info[i]['#Station Location Latitude']\n",
    "        filtered_data['Station Location Longitude'] = station_info[i]['#Station Location Longitude']\n",
    "        \n",
    "        # Convert timestamp to date\n",
    "        filtered_data['Timestamp'] = pd.to_datetime(filtered_data['#Timestamp']).dt.date\n",
    "        \n",
    "        # Rename 'Value' column to include dataset name\n",
    "        dataset_name = file_path.split('/')[1]\n",
    "        dataset_name = dataset_name.split('_')[0]\n",
    "        filtered_data.rename(columns={'Value': f'{dataset_name}'}, inplace=True)\n",
    "        \n",
    "        # Merge with combined data based on Timestamp\n",
    "        combined_data = pd.merge(combined_data, filtered_data, on=['Station Number', 'Station Location Latitude', 'Station Location Longitude', 'Timestamp'], how='outer')\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "# List of file paths for each dataset\n",
    "file_paths = ['Australia/Turbidity_Australia.csv', 'Australia/WaterTemperature_Australia.csv', 'Australia/Rainfall_Australia.csv']\n",
    "\n",
    "# Filter and combine datasets\n",
    "final_dataset = filter_and_combine(file_paths)\n",
    "\n",
    "# Reorder columns\n",
    "final_dataset = final_dataset[['Station Number', 'Station Location Latitude', 'Station Location Longitude', 'Timestamp', 'Turbidity', 'WaterTemperature', 'Rainfall']]\n",
    "\n",
    "columns_to_fill = ['WaterTemperature']\n",
    "for column in columns_to_fill:\n",
    "    final_dataset[column].fillna(final_dataset[column].mean(), inplace=True)\n",
    "\n",
    "# Replace missing values in 'Value_Rainfall' with mode\n",
    "mode_rainfall = final_dataset['Rainfall'].mode()[0]\n",
    "final_dataset['Rainfall'].fillna(mode_rainfall, inplace=True)\n",
    "# Replace missing values in 'Value_Rainfall' with mode\n",
    "\n",
    "median_turbidity = final_dataset['Turbidity'].median()\n",
    "final_dataset['Turbidity'].fillna(median_turbidity, inplace=True)\n",
    "\n",
    "# Save the final dataset\n",
    "final_dataset.to_csv('Australia/Final_Dataset_Australia.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv('Australia/Final_Dataset_Australia.csv')\n",
    "data=data.drop(columns=['Timestamp'])\n",
    "\n",
    "# Group by 'SITE_NO' and calculate the mean for each parameter\n",
    "grouped_data = data.groupby('Station Number').mean().reset_index()\n",
    "\n",
    "latitude_column = 'Station Location Latitude'  # Replace with your latitude column name\n",
    "longitude_column = 'Station Location Longitude'  # Replace with your longitude column name\n",
    "\n",
    "# Create separate heatmaps for each parameter\n",
    "parameters = ['Turbidity','WaterTemperature','Rainfall']\n",
    "for param in parameters:\n",
    "    # Create a map centered at the mean latitude and longitude of all sites\n",
    "    australia_map = folium.Map(location=[grouped_data[latitude_column].mean(), grouped_data[longitude_column].mean()], zoom_start=4)\n",
    "    \n",
    "    # Create a HeatMap layer for the parameter\n",
    "    heat_data = [[row[latitude_column], row[longitude_column], row[param]] for index, row in grouped_data.iterrows()]\n",
    "    HeatMap(heat_data, radius=10).add_to(australia_map)\n",
    "\n",
    "    # Save the map as an HTML file\n",
    "    australia_map.save(f'Australia/heatmap_{param}.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Australia/Final_Dataset_Australia.csv')\n",
    "\n",
    "# Convert Timestamp column to datetime format\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "# Plot Turbidity, Water Temperature, and Rainfall individually\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Turbidity\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(df['Timestamp'], df['Turbidity'], color='blue')\n",
    "plt.title('Turbidity')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Turbidity')\n",
    "\n",
    "# Plot Water Temperature\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(df['Timestamp'], df['WaterTemperature'], color='green')\n",
    "plt.title('Water Temperature')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "\n",
    "# Plot Rainfall\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(df['Timestamp'], df['Rainfall'], color='orange')\n",
    "plt.title('Rainfall')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Rainfall (mm)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv('Australia/Final_Dataset_Australia.csv')\n",
    "\n",
    "# Drop columns station number, latitude, and longitude\n",
    "df.drop(['Station Number', 'Station Location Latitude', 'Station Location Longitude','Timestamp'], axis=1, inplace=True)\n",
    "\n",
    "# Convert Timestamp to datetime if needed\n",
    "# df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "# Draw box plot for Turbidity\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(df['Turbidity'])\n",
    "plt.title('Box Plot of Turbidity')\n",
    "plt.ylabel('Turbidity')\n",
    "plt.show()\n",
    "\n",
    "# Draw box plot for WaterTemperature\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(df['WaterTemperature'])\n",
    "plt.title('Box Plot of Water Temperature')\n",
    "plt.ylabel('Water Temperature')\n",
    "plt.show()\n",
    "\n",
    "# Draw box plot for Rainfall\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(df['Rainfall'])\n",
    "plt.title('Box Plot of Rainfall')\n",
    "plt.ylabel('Rainfall')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and preprocess the dataset\n",
    "def load_dataset(file_path):\n",
    "    dataset = pd.read_csv(file_path)\n",
    "    # Drop unnecessary columns\n",
    "    dataset.drop(['Station Number', 'Station Location Latitude', 'Station Location Longitude'], axis=1, inplace=True)\n",
    "    return dataset\n",
    "\n",
    "# Step 2: Organize data for LSTM\n",
    "def prepare_data(dataset, target):\n",
    "    # Convert date to datetime\n",
    "    dataset['Timestamp'] = pd.to_datetime(dataset['Timestamp'])\n",
    "    # Sort by date\n",
    "    dataset.sort_values(by='Timestamp', inplace=True)\n",
    "    \n",
    "    # Extract features and target\n",
    "    features = dataset['Timestamp']\n",
    "    target_values = dataset[target]\n",
    "    \n",
    "    # Normalize target values\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    target_values_scaled = scaler.fit_transform(np.array(target_values).reshape(-1, 1))\n",
    "    \n",
    "    return features, target_values_scaled, scaler\n",
    "\n",
    "# Inside the train_lstm function, add a verbose mode to print the training loss\n",
    "def train_lstm(features, target_values_scaled):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences=True, input_shape=(features.shape[1], 1)))\n",
    "    model.add(LSTM(units=50))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    history = model.fit(features, target_values_scaled, epochs=100, batch_size=32, verbose=1)\n",
    "    # Print training loss\n",
    "    print(\"Training loss:\", history.history['loss'][-1])\n",
    "    return model\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('Australia/Final_Dataset_Australia.csv')\n",
    "\n",
    "# Define targets\n",
    "targets = ['Turbidity', 'WaterTemperature', 'Rainfall']\n",
    "models = {}\n",
    "mae_results = {}\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_size = 0.8  # 80% train, 20% test\n",
    "train_idx = int(len(dataset) * train_size)\n",
    "train_data, test_data = dataset.iloc[:train_idx], dataset.iloc[train_idx:]\n",
    "\n",
    "# Train LSTM models for each target\n",
    "for target in targets:\n",
    "    features_train, target_values_scaled_train, scaler = prepare_data(train_data, target)\n",
    "    features_train = np.array(features_train.apply(lambda x: x.value)).reshape(-1, 1, 1)\n",
    "    lstm_model = train_lstm(features_train, target_values_scaled_train)\n",
    "    models[target] = (lstm_model, scaler)\n",
    "    \n",
    "    # Prepare test data\n",
    "    features_test, target_values_scaled_test, _ = prepare_data(test_data, target)\n",
    "    features_test = np.array(features_test.apply(lambda x: x.value)).reshape(-1, 1, 1)\n",
    "    # Make predictions\n",
    "    predictions_scaled = lstm_model.predict(features_test)\n",
    "    # Inverse scaling to get actual values\n",
    "    predictions = scaler.inverse_transform(predictions_scaled)\n",
    "    # Compute MAE\n",
    "    mae = mean_absolute_error(test_data[target], predictions)\n",
    "    mae_results[target] = mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define function to predict target values\n",
    "def predict_values(model, scaler, date):\n",
    "    # Preprocess date\n",
    "    date_timestamp = datetime.timestamp(date)\n",
    "    date_array = np.array([[date_timestamp]])\n",
    "    # Normalize date\n",
    "    date_scaled = scaler.transform(date_array)\n",
    "    print(\"Scaled input:\", date_scaled)\n",
    "    # Predict target value\n",
    "    prediction_scaled = model.predict(date_scaled.reshape(1, 1, 1))\n",
    "    print(\"Scaled prediction:\", prediction_scaled)\n",
    "    # Inverse scaling to get actual value\n",
    "    prediction = scaler.inverse_transform(prediction_scaled)\n",
    "    print(\"Inverse scaled prediction:\", prediction)\n",
    "    return prediction[0][0]\n",
    "\n",
    "# Define function to predict values for a given date\n",
    "def predict_values_for_date(date):\n",
    "    predictions = {}\n",
    "    for target, (model, scaler) in models.items():\n",
    "        prediction = predict_values(model, scaler, date)\n",
    "        predictions[target] = prediction\n",
    "    return predictions\n",
    "\n",
    "# Test the function\n",
    "test_date = datetime.strptime('2012-09-05', '%Y-%m-%d')\n",
    "predicted_values = predict_values_for_date(test_date)\n",
    "print(\"Predicted values:\", predicted_values)\n",
    "\n",
    "# Print MAE results\n",
    "print(\"\\nMean Absolute Error:\",)\n",
    "for target, mae in mae_results.items():\n",
    "    print(f\"{target}: {mae}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
